---
title: NERF
author: Daniel Park
date: 2023-04-30
category: computer vision
layout: post
---

프로그램에 의해 색인 및 연구 참고용 논문들을 자동 생성합니다. 논문들은 인용으로 서로 연결되어있는 무향 그래프의 형태로 정렬됩니다.
The program automatically generates articles for indexing and research reference. Papers are arranged in an undirected graph, linked to each other by citations.

- Related Project: Private 
- Project Status: 1 Alpha
- Date: 2023-04

## Main Paper

### NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis 
- **year:** 2020 
- **url:** <https://www.semanticscholar.org/paper/428b663772dba998f5dc6a24488fff1858a0899f> 
- **abstract:** S2 TL;DR: This work describes how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrates results that outperform prior work on neural rendering and view synthesis. 
- **journal:** ArXiv 
- **volume:** abs/2003.08934 
- **pages:** null 
- **doi:** 10.1007/978-3-030-58452-8_24 
- **arxivid:** 2003.08934 

## Related Paper

### Neural Scene Representations for View Synthesis 
- **year:** 2020 
- **url:** <https://www.semanticscholar.org/paper/418bfbb63609b070bc4140b8326868b14dea4e36> 
- **abstract:** View synthesis is the problem of using a given set of input images to render a scene from new points of view. Recent approaches have combined deep learning and volume rendering to achieve photorealistic image quality. However, these methods rely on a dense 3D grid representation that only allows for a small amount of local camera movement and scales poorly to higher resolutions.In this dissertation, we present a new approach to view synthesis based on neural radiance fields, an efficient way to represent a scene as a continuous function parameterized by the weights of a neural network. In contrast to using a feed-forward neural network to predict scene properties from a small number of inputs, a neural radiance field can be directly optimized to globally reconstruct a scene from tens or hundreds of input images and thus achieve high quality novel view synthesis over a large camera baseline. The key to enabling high fidelity reconstruction of a low-dimensional signal using a neural network is a high frequency mapping of the input coordinates into a higher-dimensional space. We explain the connection between this mapping and the neural tangent kernel, and show how manipulating the frequency spectrum of the mapping provides control over the network's interpolation behavior between supervision points.

### GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields 
- **year:** 2020 
- **url:** <https://www.semanticscholar.org/paper/69a1d72bac9dfb18940ff97ae91643d6c8158e6d> 
- **abstract:** Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects’ shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose. 
- **journal:** 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 
- **volume:** null 
- **pages:** 11448-11459 
- **doi:** 10.1109/CVPR46437.2021.01129 
- **arxivid:** 2011.12100 

### Occupancy Networks: Learning 3D Reconstruction in Function Space 
- **year:** 2018 
- **url:** <https://www.semanticscholar.org/paper/2e689bdce24cf3644432505ce2783f03a1445ed2> 
- **abstract:** With the advent of deep neural networks, learning-based approaches for 3D reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3D reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3D reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks. 
- **journal:** 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 
- **volume:** null 
- **pages:** 4455-4465 
- **doi:** 10.1109/CVPR.2019.00459 
- **arxivid:** 1812.03828 

### DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation 
- **year:** 2019 
- **url:** <https://www.semanticscholar.org/paper/dd81523b9accdf1c13cd37f76b22ab27d84b7a42> 
- **abstract:** Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape's surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape's boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF's both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work. 
- **journal:** 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 
- **volume:** null 
- **pages:** 165-174 
- **doi:** 10.1109/CVPR.2019.00025 
- **arxivid:** 1901.05103 

### Nerfies: Deformable Neural Radiance Fields 
- **year:** 2020 
- **url:** <https://www.semanticscholar.org/paper/0f1af3f94f4699cd70a554f68f8f9e2c8e3d53dd> 
- **abstract:** We present the first method capable of photorealistically reconstructing deformable scenes using photos/videos captured casually from mobile phones. Our approach augments neural radiance fields (NeRF) by optimizing an additional continuous volumetric deformation field that warps each observed point into a canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone to local minima, and propose a coarse-to-fine optimization method for coordinate-based models that allows for more robust optimization. By adapting principles from geometry processing and physical simulation to NeRF-like models, we propose an elastic regularization of the deformation field that further improves robustness. We show that our method can turn casually captured selfie photos/videos into deformable NeRF models that allow for photorealistic renderings of the subject from arbitrary viewpoints, which we dub "nerfies." We evaluate our method by collecting time-synchronized data using a rig with two mobile phones, yielding train/validation images of the same pose at different viewpoints. We show that our method faithfully reconstructs non-rigidly deforming scenes and reproduces unseen views with high fidelity. 
- **journal:** 2021 IEEE/CVF International Conference on Computer Vision (ICCV) 
- **volume:** null 
- **pages:** 5845-5854 
- **doi:** 10.1109/ICCV48922.2021.00581 
- **arxivid:** 2011.12948 

### Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines 
- **year:** 2019 
- **url:** <https://www.semanticscholar.org/paper/24f3accec5e5c605c006743450bb12cd997bf0c4> 
- **abstract:** We present a practical and robust deep learning solution for capturing and rendering novel views of complex real world scenes for virtual exploration. Previous approaches either require intractably dense view sampling or provide little to no guidance for how users should sample views of a scene to reliably render high-quality novel views. Instead, we propose an algorithm for view synthesis from an irregular grid of sampled views that first expands each sampled view into a local light field via a multiplane image (MPI) scene representation, then renders novel views by blending adjacent local light fields. We extend traditional plenoptic sampling theory to derive a bound that specifies precisely how densely users should sample views of a given scene when using our algorithm. In practice, we apply this bound to capture and render views of real world scenes that achieve the perceptual quality of Nyquist rate view sampling while using up to 4000x fewer views. We demonstrate our approach's practicality with an augmented reality smartphone app that guides users to capture input images of a scene and viewers that enable realtime virtual exploration on desktop and mobile platforms. 
- **journal:** arXiv: Computer Vision and Pattern Recognition 
- **volume:**  
- **pages:** null 

### FastNeRF: High-Fidelity Neural Rendering at 200FPS 
- **year:** 2021 
- **url:** <https://www.semanticscholar.org/paper/b99c59e6601e2cc5ba1cb2dd740c6c5487b77fe6> 
- **abstract:** Recent work on Neural Radiance Fields (NeRF) showed how neural networks can be used to encode complex 3D environments that can be rendered photorealistically from novel viewpoints. Rendering these images is very computationally demanding and recent improvements are still a long way from enabling interactive rates, even on high-end hardware. Motivated by scenarios on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based system capable of rendering high fidelity photorealistic images at 200Hz on a high-end consumer GPU. The core of our method is a graphics-inspired factorization that allows for (i) compactly caching a deep radiance map at each position in space, (ii) efficiently querying that map using ray directions to estimate the pixel values in the rendered image. Extensive experiments show that the proposed method is 3000 times faster than the original NeRF algorithm and at least an order of magnitude faster than existing work on accelerating NeRF, while maintaining visual quality and extensibility. 
- **journal:** 2021 IEEE/CVF International Conference on Computer Vision (ICCV) 
- **volume:** null 
- **pages:** 14326-14335 
- **doi:** 10.1109/ICCV48922.2021.01408 
- **arxivid:** 2103.10380 

### Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations 
- **year:** 2019 
- **url:** <https://www.semanticscholar.org/paper/b9d4a1ac5e41570082828b405b289aa6959252a4> 
- **abstract:** Unsupervised learning with generative models has the potential of discovering rich representations of 3D scenes. While geometric deep learning has explored 3D-structure-aware representations of scene geometry, these models typically require explicit 3D supervision. Emerging neural scene representations can be trained only with posed 2D images, but existing methods ignore the three-dimensional structure of scenes. We propose Scene Representation Networks (SRNs), a continuous, 3D-structure-aware scene representation that encodes both geometry and appearance. SRNs represent scenes as continuous functions that map world coordinates to a feature representation of local scene properties. By formulating the image formation as a differentiable ray-marching algorithm, SRNs can be trained end-to-end from only 2D images and their camera poses, without access to depth or shape. This formulation naturally generalizes across scenes, learning powerful geometry and appearance priors in the process. We demonstrate the potential of SRNs by evaluating them for novel view synthesis, few-shot reconstruction, joint shape and appearance interpolation, and unsupervised discovery of a non-rigid face model. 
- **journal:** ArXiv 
- **volume:** abs/1906.01618 
- **pages:** null 
- **arxivid:** 1906.01618 

### MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo 
- **year:** 2021 
- **url:** <https://www.semanticscholar.org/paper/169971b60749264cbbe2b577dc4d2ad23ca4f46c> 
- **abstract:** We present MVSNeRF, a novel neural rendering approach that can efficiently reconstruct neural radiance fields for view synthesis. Unlike prior works on neural radiance fields that consider per-scene optimization on densely captured images, we propose a generic deep neural network that can reconstruct radiance fields from only three nearby input views via fast network inference. Our approach leverages plane-swept cost volumes (widely used in multi-view stereo) for geometry-aware scene reasoning, and combines this with physically based volume rendering for neural radiance field reconstruction. We train our network on real objects in the DTU dataset, and test it on three different datasets to evaluate its effectiveness and generalizability. Our approach can generalize across scenes (even indoor scenes, completely different from our training scenes of objects) and generate realistic view synthesis results using only three input images, significantly outperforming concurrent works on generalizable radiance field reconstruction. Moreover, if dense images are captured, our estimated radiance field representation can be easily fine-tuned; this leads to fast per-scene reconstruction with higher rendering quality and substantially less optimization time than NeRF. 
- **journal:** 2021 IEEE/CVF International Conference on Computer Vision (ICCV) 
- **volume:** null 
- **pages:** 14104-14113 
- **doi:** 10.1109/ICCV48922.2021.01386 
- **arxivid:** 2103.15595 

### D-NeRF: Neural Radiance Fields for Dynamic Scenes 
- **year:** 2020 
- **url:** <https://www.semanticscholar.org/paper/694bdf6e5906992dad2987a3cc8d1a176de691c9> 
- **abstract:** Neural rendering techniques combining machine learning with geometric reasoning have arisen as one of the most promising approaches for synthesizing novel views of a scene from a sparse set of images. Among these, stands out the Neural radiance fields (NeRF) [31], which trains a deep network to map 5D input coordinates (representing spatial location and viewing direction) into a volume density and view-dependent emitted radiance. However, despite achieving an unprecedented level of photorealism on the generated images, NeRF is only applicable to static scenes, where the same spatial location can be queried from different images. In this paper we introduce D-NeRF, a method that extends neural radiance fields to a dynamic domain, allowing to reconstruct and render novel images of objects under rigid and non-rigid motions from a single camera moving around the scene. For this purpose we consider time as an additional input to the system, and split the learning process in two main stages: one that encodes the scene into a canonical space and another that maps this canonical representation into the deformed scene at a particular time. Both mappings are simultaneously learned using fully-connected networks. Once the networks are trained, D-NeRF can render novel images, controlling both the camera view and the time variable, and thus, the object movement. We demonstrate the effectiveness of our approach on scenes with objects under rigid, articulated and non-rigid motions. Code, model weights and the dynamic scenes dataset will be available at [1]. 
- **journal:** 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 
- **volume:** null 
- **pages:** 10313-10322 
- **doi:** 10.1109/CVPR46437.2021.01018 
- **arxivid:** 2011.13961 

### Space-time Neural Irradiance Fields for Free-Viewpoint Video 
- **year:** 2020 
- **url:** <https://www.semanticscholar.org/paper/414da5e0716eb4707e033b7967d7671dfe71ab71> 
- **abstract:** We present a method that learns a spatiotemporal neural irradiance field for dynamic scenes from a single video. Our learned representation enables free-viewpoint rendering of the input video. Our method builds upon recent advances in implicit representations. Learning a spatiotemporal irradiance field from a single video poses significant challenges because the video contains only one observation of the scene at any point in time. The 3D geometry of a scene can be legitimately represented in numerous ways since varying geometry (motion) can be explained with varying appearance and vice versa. We address this ambiguity by constraining the time-varying geometry of our dynamic scene representation using the scene depth estimated from video depth estimation methods, aggregating contents from individual frames into a single global representation. We provide an extensive quantitative evaluation and demonstrate compelling free-viewpoint rendering results. 
- **journal:** 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 
- **volume:** null 
- **pages:** 9416-9426 
- **doi:** 10.1109/CVPR46437.2021.00930 
- **arxivid:** 2011.12950 

### NeRF++: Analyzing and Improving Neural Radiance Fields 
- **year:** 2020 
- **url:** <https://www.semanticscholar.org/paper/5b0ea2c92ee16fa2f5a3dbc9315cd5c1e4ec1d88> 
- **abstract:** Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360 capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF fits multi-layer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF's success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360 captures of objects within large-scale, unbounded 3D scenes. Our method improves view synthesis fidelity in this challenging scenario. Code is available at this https URL. 
- **journal:** ArXiv 
- **volume:** abs/2010.07492 
- **pages:** null 
- **arxivid:** 2010.07492 

### Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans 
- **year:** 2020 
- **url:** <https://www.semanticscholar.org/paper/af8faec7c0b8f4b2a28d42a86e0e7d499016c560> 
- **abstract:** This paper addresses the challenge of novel view synthesis for a human performer from a very sparse set of camera views. Some recent works have shown that learning implicit neural representations of 3D scenes achieves remarkable view synthesis quality given dense input views. However, the representation learning will be ill-posed if the views are highly sparse. To solve this ill-posed problem, our key idea is to integrate observations over video frames. To this end, we propose Neural Body, a new human body representation which assumes that the learned neural representations at different frames share the same set of latent codes anchored to a deformable mesh, so that the observations across frames can be naturally integrated. The deformable mesh also provides geometric guidance for the network to learn 3D representations more efficiently. To evaluate our approach, we create a multi-view dataset named ZJU-MoCap that captures performers with complex motions. Experiments on ZJU-MoCap show that our approach outperforms prior works by a large margin in terms of novel view synthesis quality. We also demonstrate the capability of our approach to reconstruct a moving person from a monocular video on the People-Snapshot dataset. 
- **journal:** 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 
- **volume:** null 
- **pages:** 9050-9059 
- **doi:** 10.1109/CVPR46437.2021.00894 
- **arxivid:** 2012.15838 

### Neural Sparse Voxel Fields 
- **year:** 2020 
- **url:** <https://www.semanticscholar.org/paper/17d7767a6ea87f4ab24d9cfaa5039160af9cad76> 
- **abstract:** Photo-realistic free-viewpoint rendering of real-world scenes using classical computer graphics techniques is challenging, because it requires the difficult step of capturing detailed appearance and geometry models. Recent studies have demonstrated promising results by learning scene representations that implicitly encode both geometry and appearance without 3D supervision. However, existing approaches in practice often show blurry renderings caused by the limited network capacity or the difficulty in finding accurate intersections of camera rays with the scene geometry. Synthesizing high-resolution imagery from these representations often requires time-consuming optical ray marching. In this work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene representation for fast and high-quality free-viewpoint rendering. NSVF defines a set of voxel-bounded implicit fields organized in a sparse voxel octree to model local properties in each cell. We progressively learn the underlying voxel structures with a diffentiable ray-marching operation from only a set of posed RGB images. With the sparse voxel octree structure, rendering novel views can be accelerated by skipping the voxels containing no relevant scene content. Our method is over 10 times faster than the state-of-the-art (namely, NeRF) at inference time while achieving higher quality results. Furthermore, by utilizing an explicit sparse voxel representation, our method can easily be applied to scene editing and scene composition. We also demonstrate several challenging tasks, including multi-scene learning, free-viewpoint rendering of a moving human, and large-scale scene rendering. 
- **journal:** ArXiv 
- **volume:** abs/2007.11571 
- **pages:** null 
- **arxivid:** 2007.11571 

### Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance 
- **year:** 2020 
- **url:** <https://www.semanticscholar.org/paper/2854af340fffab4f6de0bcdcf21d1d33ece08ae8> 
- **abstract:** In this work we address the challenging problem of multiview 3D surface reconstruction. We introduce a neural network architecture that simultaneously learns the unknown geometry, camera parameters, and a neural renderer that approximates the light reflected from the surface towards the camera. The geometry is represented as a zero level-set of a neural network, while the neural renderer, derived from the rendering equation, is capable of (implicitly) modeling a wide set of lighting conditions and materials. We trained our network on real world 2D images of objects with different material properties, lighting conditions, and noisy camera initializations from the DTU MVS dataset. We found our model to produce state of the art 3D surface reconstructions with high fidelity, resolution and detail. 
- **journal:** arXiv: Computer Vision and Pattern Recognition 
- **volume:**  null
- **pages:** null 

### PlenOctrees for Real-time Rendering of Neural Radiance Fields 
- **year:** 2021 
- **url:** <https://www.semanticscholar.org/paper/5744fcc21b40327f7ad710de7d947d4584c53012> 
- **abstract:** We introduce a method to render Neural Radiance Fields (NeRFs) in real time using PlenOctrees, an octree-based 3D representation which supports view-dependent effects. Our method can render 800×800 images at more than 150 FPS, which is over 3000 times faster than conventional NeRFs. We do so without sacrificing quality while preserving the ability of NeRFs to perform free-viewpoint rendering of scenes with arbitrary geometry and view-dependent effects. Real-time performance is achieved by pre-tabulating the NeRF into a PlenOctree. In order to preserve view-dependent effects such as specularities, we factorize the appearance via closed-form spherical basis functions. Specifically, we show that it is possible to train NeRFs to predict a spherical harmonic representation of radiance, removing the viewing direction as an input to the neural network. Furthermore, we show that PlenOctrees can be directly optimized to further minimize the reconstruction loss, which leads to equal or better quality compared to competing methods. Moreover, this octree optimization step can be used to reduce the training time, as we no longer need to wait for the NeRF training to converge fully. Our real-time neural rendering approach may potentially enable new applications such as 6-DOF industrial and product visualizations, as well as next generation AR/VR systems. PlenOctrees are amenable to in-browser rendering as well; please visit the project page for the interactive online demo, as well as video and code: <https://alexyu.net/plenoctrees.> 
- **journal:** 2021 IEEE/CVF International Conference on Computer Vision (ICCV) 
- **volume:** null 
- **pages:** 5732-5741 
- **doi:** 10.1109/ICCV48922.2021.00570 
- **arxivid:** 2103.14024 

### KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs 
- **year:** 2021 
- **url:** <https://www.semanticscholar.org/paper/c041aaed581616e122e790dd2769337216df3d8d> 
- **abstract:** NeRF synthesizes novel views of a scene with unprecedented quality by fitting a neural radiance field to RGB images. However, NeRF requires querying a deep Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering times, even on modern GPUs. In this paper, we demonstrate that real-time rendering is possible by utilizing thousands of tiny MLPs instead of one single large MLP. In our setting, each individual MLP only needs to represent parts of the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining this divide-and-conquer strategy with further optimizations, rendering is accelerated by three orders of magnitude compared to the original NeRF model without incurring high storage costs. Further, using teacher-student distillation for training, we show that this speed-up can be achieved without sacrificing visual quality. 
- **journal:** 2021 IEEE/CVF International Conference on Computer Vision (ICCV) 
- **volume:** null 
- **pages:** 14315-14325 
- **doi:** 10.1109/ICCV48922.2021.01407 
- **arxivid:** 2103.13744 

### Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields 
- **year:** 2021 
- **url:** <https://www.semanticscholar.org/paper/21336e57dc2ab9ae2171a0f6c35f7d1aba584796> 
- **abstract:** The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call "mip-NeRF" (à la "mipmap"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF’s ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22× faster. 
- **journal:** 2021 IEEE/CVF International Conference on Computer Vision (ICCV) 
- **volume:** null 
- **pages:** 5835-5844 
- **doi:** 10.1109/ICCV48922.2021.00580 
- **arxivid:** 2103.13415 

### Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction 
- **year:** 2021 
- **url:** <https://www.semanticscholar.org/paper/4f7eb65f8d3c1eeb97e30f7ac68977ff16e1e942> 
- **abstract:** We present a super-fast convergence approach to reconstructing the per-scene radiance field from a set of images that capture the scene with known poses. This task, which is often applied to novel view synthesis, is recently revolution-ized by Neural Radiance Field (NeRF) for its state-of-the-art quality and fiexibility. However, NeRF and its variants require a lengthy training time ranging from hours to days for a single scene. In contrast, our approach achieves NeRF-comparable quality and converges rapidly from scratch in less than 15 minutes with a single GPU. We adopt a representation consisting of a density voxel grid for scene geometry and a feature voxel grid with a shallow network for complex view-dependent appearance. Modeling with explicit and discretized volume representations is not new, but we propose two simple yet non-trivial techniques that contribute to fast convergence speed and high-quality output. First, we introduce the post-activation interpolation on voxel density, which is capable of producing sharp surfaces in lower grid resolution. Second, direct voxel density optimization is prone to suboptimal geometry solutions, so we robustify the optimization process by imposing several priors. Finally, evaluation on five inward-facing benchmarks shows that our method matches, if not surpasses, NeRF's quality, yet it only takes about 15 minutes to train from scratch for a new scene. Code: <https://github.com/sunset1995/DirectVoxGO.> 
- **journal:** 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 
- **volume:** null 
- **pages:** 5449-5459 
- **doi:** 10.1109/CVPR52688.2022.00538 
- **arxivid:** 2111.11215 

### DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction 
- **year:** 2019 
- **url:** <https://www.semanticscholar.org/paper/83d858045fcad929f0422b22a89759c1dea48dcb> 
- **abstract:** Reconstructing 3D shapes from single-view images has been a long-standing research problem. In this paper, we present DISN, a Deep Implicit Surface Network which can generate a high-quality detail-rich 3D mesh from an 2D image by predicting the underlying signed distance fields. In addition to utilizing global image features, DISN predicts the projected location for each 3D point on the 2D image, and extracts local features from the image feature maps. Combining global and local features significantly improves the accuracy of the signed distance field prediction, especially for the detail-rich areas. To the best of our knowledge, DISN is the first method that constantly captures details such as holes and thin structures present in 3D shapes from single-view images. DISN achieves the state-of-the-art single-view reconstruction performance on a variety of shape categories reconstructed from both synthetic and real images. Code is available at this https URL The supplementary can be found at this https URL 
- **journal:** ArXiv 
- **volume:** abs/1905.10711 
- **pages:** 490-500 
- **arxivid:** 1905.10711 

### GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis 
- **year:** 2020 
- **url:** <https://www.semanticscholar.org/paper/62d337dbaead376ca042f23d62c0d4b65ec98546> 
- **abstract:** While 2D generative adversarial networks have enabled high-resolution image synthesis, they largely lack an understanding of the 3D world and the image formation process. Thus, they do not provide precise control over camera viewpoint or object pose. To address this problem, several recent approaches leverage intermediate voxel-based representations in combination with differentiable rendering. However, existing methods either produce low image resolution or fall short in disentangling camera and scene properties, e.g., the object identity may vary with the viewpoint. In this paper, we propose a generative model for radiance fields which have recently proven successful for novel view synthesis of a single scene. In contrast to voxel-based representations, radiance fields are not confined to a coarse discretization of the 3D space, yet allow for disentangling camera and scene properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training our model from unposed 2D images alone. We systematically analyze our approach on several challenging synthetic and real-world datasets. Our experiments reveal that radiance fields are a powerful representation for generative image synthesis, leading to 3D consistent models that render with high fidelity. 
- **journal:** ArXiv 
- **volume:** abs/2007.02442 
- **pages:** null 
- **arxivid:** 2007.02442 

### Plenoxels: Radiance Fields without Neural Networks 
- **year:** 2021 
- **url:** <https://www.semanticscholar.org/paper/e91f73aaef155391b5b07e6612f5346dea888f64> 
- **abstract:** We introduce Plenoxels (plenoptic voxels), a systemfor photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see <https://alexyu.net/plenoxels.> 
- **journal:** 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 
- **volume:** null 
- **pages:** 5491-5500 
- **doi:** 10.1109/CVPR52688.2022.00542 
- **arxivid:** 2112.05131 

### pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis 
- **year:** 2020 
- **url:** <https://www.semanticscholar.org/paper/44e6a301714a17b05908da426411b43fa3ebedf0> 
- **abstract:** We have witnessed rapid progress on 3D-aware image synthesis, leveraging recent advances in generative visual models and neural rendering. Existing approaches how-ever fall short in two ways: first, they may lack an under-lying 3D representation or rely on view-inconsistent rendering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality. We propose a novel generative model, named Periodic Implicit Generative Adversarial Networks (π-GAN or pi-GAN), for high-quality 3D-aware image synthesis. π-GAN leverages neural representations with periodic activation functions and volumetric rendering to represent scenes as view-consistent radiance fields. The proposed approach obtains state-of-the-art results for 3D-aware image synthesis with multiple real and synthetic datasets. 
- **journal:** 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 
- **volume:** null 
- **pages:** 5795-5805 
- **doi:** 10.1109/CVPR46437.2021.00574 
- **arxivid:** 2012.00926 

### Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video 
- **year:** 2020 
- **url:** <https://www.semanticscholar.org/paper/0876926054f5474f04e123f1aad2e44377cc7428> 
- **abstract:** We present Non-Rigid Neural Radiance Fields (NR-NeRF), a reconstruction and novel view synthesis approach for general non-rigid dynamic scenes. Our approach takes RGB images of a dynamic scene as input (e.g., from a monocular video recording), and creates a high-quality space-time geometry and appearance representation. We show that a single handheld consumer-grade camera is sufficient to synthesize sophisticated renderings of a dynamic scene from novel virtual camera views, e.g. a ‘bullet-time’ video effect. NR-NeRF disentangles the dynamic scene into a canonical volume and its deformation. Scene deformation is implemented as ray bending, where straight rays are deformed non-rigidly. We also propose a novel rigidity network to better constrain rigid regions of the scene, leading to more stable results. The ray bending and rigidity network are trained without explicit supervision. Our formulation enables dense correspondence estimation across views and time, and compelling video editing applications such as motion exaggeration. Our code will be open sourced. 
- **journal:** 2021 IEEE/CVF International Conference on Computer Vision (ICCV) 
- **volume:** null 
- **pages:** 12939-12950 
- **doi:** 10.1109/ICCV48922.2021.01272 
- **arxivid:** 2012.12247 

### NeX: Real-time View Synthesis with Neural Basis Expansion 
- **year:** 2021 
- **url:** <https://www.semanticscholar.org/paper/0283d3ace44c0deeb9c56039989eeb072c54f825> 
- **abstract:** We present NeX, a new approach to novel view synthesis based on enhancements of multiplane image (MPI) that can reproduce next-level view-dependent effects—in real time. Unlike traditional MPI that uses a set of simple RGBα planes, our technique models view-dependent effects by instead parameterizing each pixel as a linear combination of basis functions learned from a neural network. Moreover, we propose a hybrid implicit-explicit modeling strategy that improves upon fine detail and produces state-of-the-art results. Our method is evaluated on benchmark forwardfacing datasets as well as our newly-introduced dataset designed to test the limit of view-dependent modeling with significantly more challenging effects such as the rainbow reflections on a CD. Our method achieves the best overall scores across all major metrics on these datasets with more than 1000× faster rendering time than the state of the art. For real-time demos, visit <https://nex-mpi.github.io/> 
- **journal:** 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 
- **volume:** null 
- **pages:** 8530-8539 
- **doi:** 10.1109/CVPR46437.2021.00843 
- **arxivid:** 2103.05606 

### pixelNeRF: Neural Radiance Fields from One or Few Images 
- **year:** 2020 
- **url:** <https://www.semanticscholar.org/paper/4365f51fc270c55005adb794002685078a6fca1d> 
- **abstract:** We propose pixelNeRF, a learning framework that predicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance fields [27] involves optimizing the representation to every scene independently, requiring many calibrated views and significant compute time. We take a step towards resolving these shortcomings by introducing an architecture that conditions a NeRF on image inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). Leveraging the volume rendering approach of NeRF, our model can be trained directly from images with no explicit 3D supervision. We conduct extensive experiments on ShapeNet benchmarks for single image novel view synthesis tasks with held-out objects as well as entire unseen categories. We further demonstrate the flexibility of pixelNeRF by demonstrating it on multi-object ShapeNet scenes and real scenes from the DTU dataset. In all cases, pixelNeRF outperforms current state-of-the-art baselines for novel view synthesis and single image 3D reconstruction. For the video and code, please visit the project website:<https://alexyu.net/pixelnerf.> 
- **journal:** 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 
- **volume:** null 
- **pages:** 4576-4585 
- **doi:** 10.1109/CVPR46437.2021.00455 
- **arxivid:** 2012.02190 

### Implicit Neural Representations with Periodic Activation Functions 
- **year:** 2020 
- **url:** <https://www.semanticscholar.org/paper/43b1e34451f783fed053c1d539d7560dc4ec16a9> 
- **abstract:** Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with hypernetworks to learn priors over the space of Siren functions. 
- **journal:** ArXiv 
- **volume:** abs/2006.09661 
- **pages:** null 
- **arxivid:** 2006.09661 

### NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections 
- **year:** 2020 
- **url:** <https://www.semanticscholar.org/paper/691eddbfaebbc71f6a12d3c99d5c155042459434> 
- **abstract:** We present a learning-based method for synthesizing novel views of complex scenes using only unstructured collections of in-the-wild photographs. We build on Neural Radiance Fields (NeRF), which uses the weights of a multi-layer perceptron to model the density and color of a scene as a function of 3D coordinates. While NeRF works well on images of static subjects captured under controlled settings, it is incapable of modeling many ubiquitous, real-world phenomena in uncontrolled images, such as variable illumination or transient occluders. We introduce a series of extensions to NeRF to address these issues, thereby enabling accurate reconstructions from unstructured image collections taken from the internet. We apply our system, dubbed NeRF-W, to internet photo collections of famous landmarks, and demonstrate temporally consistent novel view renderings that are significantly closer to photorealism than the prior state of the art. 
- **journal:** 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 
- **volume:** null 
- **pages:** 7206-7215 
- **doi:** 10.1109/CVPR46437.2021.00713 
- **arxivid:** 2008.02268 

### Convolutional Occupancy Networks 
- **year:** 2020 
- **url:** <https://www.semanticscholar.org/paper/5a6732513a1dc0bea059543f208a7556e3e31067> 
- **abstract:** S2 TL;DR: Convolutional Occupancy Networks is proposed, a more flexible implicit representation for detailed reconstruction of objects and 3D scenes that enables the fine-grained implicit 3D reconstruction of single objects, scales to large indoor scenes, and generalizes well from synthetic to real data. 
- **journal:** ArXiv 
- **volume:** abs/2003.04618 
- **pages:** null 
- **doi:** 10.1007/978-3-030-58580-8_31 
- **arxivid:** 2003.04618 

### IBRNet: Learning Multi-View Image-Based Rendering 
- **year:** 2021 
- **url:** <https://www.semanticscholar.org/paper/7cbc3dd0280b8c4551ac934af42dc227d43754f7> 
- **abstract:** We present a method that synthesizes novel views of complex scenes by interpolating a sparse set of nearby views. The core of our method is a network architecture that includes a multilayer perceptron and a ray transformer that estimates radiance and volume density at continuous 5D locations (3D spatial locations and 2D viewing directions), drawing appearance information on the fly from multiple source views. By drawing on source views at render time, our method hearkens back to classic work on image-based rendering (IBR), and allows us to render high-resolution imagery. Unlike neural scene representation work that optimizes per-scene functions for rendering, we learn a generic view interpolation function that generalizes to novel scenes. We render images using classic volume rendering, which is fully differentiable and allows us to train using only multi-view posed images as supervision. Experiments show that our method outperforms recent novel view synthesis methods that also seek to generalize to novel scenes. Further, if fine-tuned on each scene, our method is competitive with state-of-the-art single-scene neural rendering methods.1 
- **journal:** 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 
- **volume:** null 
- **pages:** 4688-4697 
- **doi:** 10.1109/CVPR46437.2021.00466 
- **arxivid:** 2102.13090 

### SELF-SUPERVISED SCENE REPRESENTATION LEARNING A DISSERTATION SUBMITTED TO THE DEPARTMENT OF ELECTRICAL ENGINEERING AND THE COMMITTEE ON GRADUATE STUDIES OF STANFORD UNIVERSITY IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE DEGREE OF DOCTOR OF PHILOSOPHY 
- **year:** 2020 
- **url:** <https://www.semanticscholar.org/paper/4560f97f0ea4d0cdc9c93a3ba6c94d5d15438a5b> 
- **abstract:** null 

### DeepVoxels: Learning Persistent 3D Feature Embeddings 
- **year:** 2018 
- **url:** <https://www.semanticscholar.org/paper/e2d3abc7008a269880918ee7d903a55d06acdd55> 
- **abstract:** In this work, we address the lack of 3D understanding of generative neural networks by introducing a persistent 3D feature embedding for view synthesis. To this end, we propose DeepVoxels, a learned representation that encodes the view-dependent appearance of a 3D scene without having to explicitly model its geometry. At its core, our approach is based on a Cartesian 3D grid of persistent embedded features that learn to make use of the underlying 3D scene structure. Our approach combines insights from 3D geometric computer vision with recent advances in learning image-to-image mappings based on adversarial loss functions. DeepVoxels is supervised, without requiring a 3D reconstruction of the scene, using a 2D re-rendering loss and enforces perspective and multi-view geometry in a principled manner. We apply our persistent 3D scene representation to the problem of novel view synthesis demonstrating high-quality results for a variety of challenging scenes. 
- **journal:** 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 
- **volume:** null 
- **pages:** 2432-2441 
- **doi:** 10.1109/CVPR.2019.00254 
- **arxivid:** 1812.01024 

### Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains 
- **year:** 2020 
- **url:** <https://www.semanticscholar.org/paper/a0dc3135c40e150f0271002a96b7c9680b6cac40> 
- **abstract:** We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities. 
- **journal:** ArXiv 
- **volume:** abs/2006.10739 
- **pages:** null 
- **arxivid:** 2006.10739 

### MINE: Towards Continuous Depth MPI with NeRF for Novel View Synthesis 
- **year:** 2021 
- **url:** <https://www.semanticscholar.org/paper/0f138a10bed7fb1266f74d6a02407f9c43b76b6f> 
- **abstract:** In this paper, we propose MINE to perform novel view synthesis and depth estimation via dense 3D reconstruction from a single image. Our approach is a continuous depth generalization of the Multiplane Images (MPI) by introducing the NEural radiance fields (NeRF). Given a single image as input, MINE predicts a 4-channel image (RGB and volume density) at arbitrary depth values to jointly reconstruct the camera frustum and fill in occluded contents. The reconstructed and inpainted frustum can then be easily rendered into novel RGB or depth views using differentiable rendering. Extensive experiments on RealEstate10K, KITTI and Flowers Light Fields show that our MINE outperforms state-of-the-art by a large margin in novel view synthesis. We also achieve competitive results in depth estimation on iBims-1 and NYU-v2 without annotated depth supervision. Our source code is available at <https://github.com/vincentfung13/MINE.> 
- **journal:** 2021 IEEE/CVF International Conference on Computer Vision (ICCV) 
- **volume:** null 
- **pages:** 12558-12568 
- **doi:** 10.1109/ICCV48922.2021.01235 
- **arxivid:** 2103.14910 

### Differentiable Volumetric Rendering: Learning Implicit 3D Representations Without 3D Supervision 
- **year:** 2019 
- **url:** <https://www.semanticscholar.org/paper/dfc66041b88ccdfc21ae816e0ecee9e8d1ef4731> 
- **abstract:** Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from RGB images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from RGB images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes. 
- **journal:** 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 
- **volume:** null 
- **pages:** 3501-3512 
- **doi:** 10.1109/cvpr42600.2020.00356 
- **arxivid:** 1912.07372 

### Baking Neural Radiance Fields for Real-Time View Synthesis 
- **year:** 2021 
- **url:** <https://www.semanticscholar.org/paper/8bd70d3dcfa295d9710922c34c1a9eeb0be48b94> 
- **abstract:** Neural volumetric representations such as Neural Radiance Fields (NeRF) have emerged as a compelling technique for learning to represent 3D scenes from images with the goal of rendering photorealistic images of the scene from unobserved viewpoints. However, NeRF’s computational requirements are prohibitive for real-time applications: rendering views from a trained NeRF requires querying a multilayer perceptron (MLP) hundreds of times per ray. We present a method to train a NeRF, then precompute and store (i.e. "bake") it as a novel representation called a Sparse Neural Radiance Grid (SNeRG) that enables real-time rendering on commodity hardware. To achieve this, we introduce 1) a reformulation of NeRF’s architecture, and 2) a sparse voxel grid representation with learned feature vectors. The resulting scene representation retains NeRF’s ability to render fine geometric details and view-dependent appearance, is compact (averaging less than 90 MB per scene), and can be rendered in real-time (higher than 30 frames per second on a laptop GPU). Actual screen captures are shown in our video. 
- **journal:** 2021 IEEE/CVF International Conference on Computer Vision (ICCV) 
- **volume:** null 
- **pages:** 5855-5864 
- **doi:** 10.1109/ICCV48922.2021.00582 
- **arxivid:** 2103.14645 

### Deep Local Shapes: Learning Local SDF Priors for Detailed 3D Reconstruction 
- **year:** 2020 
- **url:** <https://www.semanticscholar.org/paper/ce68c019f2b522ac6a4f7fed7c3d3bf26b00d0d9> 
- **abstract:** S2 TL;DR: This work introduces Deep Local Shapes (DeepLS), a deep shape representation that enables encoding and reconstruction of high-quality 3D shapes without prohibitive memory requirements, and demonstrates the effectiveness and generalization power of this representation. 
- **journal:** ArXiv 
- **volume:** abs/2003.10983 
- **pages:** 608-625 
- **doi:** 10.1007/978-3-030-58526-6_36 
- **arxivid:** 2003.10983 

### Free View Synthesis 
- **year:** 2020 
- **url:** <https://www.semanticscholar.org/paper/49fae04a4e9383080788759f63dba75c86bd21b0> 
- **abstract:** S2 TL;DR: This work presents a method for novel view synthesis from input images that are freely distributed around a scene that can synthesize images for free camera movement through the scene, and works for general scenes with unconstrained geometric layouts. 
- **journal:** ArXiv 
- **volume:** abs/2008.05511 
- **pages:** null 
- **doi:** 10.1007/978-3-030-58529-7_37 
- **arxivid:** 2008.05511 

### Instant neural graphics primitives with a multiresolution hash encoding 
- **year:** 2022 
- **url:** <https://www.semanticscholar.org/paper/60e69982ef2920596c6f31d6fd3ca5e9591f3db6> 
- **abstract:** Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920×1080. 
- **journal:** ACM Transactions on Graphics (TOG) 
- **volume:** 41 
- **pages:** 1 - 15 
- **doi:** 10.1145/3528223.3530127 
- **arxivid:** 2201.05989 

### Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes 
- **year:** 2020 
- **url:** <https://www.semanticscholar.org/paper/13034a395d5c6728c9b11e777828d9998018cbf6> 
- **abstract:** We present a method to perform novel view and time synthesis of dynamic scenes, requiring only a monocular video with known camera poses as input. To do this, we introduce Neural Scene Flow Fields, a new representation that models the dynamic scene as a time-variant continuous function of appearance, geometry, and 3D scene motion. Our representation is optimized through a neural network to fit the observed input views. We show that our representation can be used for varieties of in-the-wild scenes, including thin structures, view-dependent effects, and complex degrees of motion. We conduct a number of experiments that demonstrate our approach significantly outperforms recent monocular view synthesis methods, and show qualitative results of space-time view synthesis on a variety of real-world videos. 
- **journal:** 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 
- **volume:** null 
- **pages:** 6494-6504 
- **doi:** 10.1109/CVPR46437.2021.00643 
- **arxivid:** 2011.13084 
